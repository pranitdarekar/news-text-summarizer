{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6jKG5xbgf4T",
        "outputId": "ae4bf109-8783-4ad4-ee05-04d277270180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, Attention\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH6Y_Q1mtmN0",
        "outputId": "397de215-4e3f-4574-953d-28a3ab2e1743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KkamxyPCu2bi"
      },
      "outputs": [],
      "source": [
        "def remove_missing_rows(dataset_path, cleaned_dataset_path):\n",
        "  # load the dataset\n",
        "  data_frame = pd.read_csv(dataset_path)\n",
        "\n",
        "  # drop the rows with missing values\n",
        "  data_frame.dropna(subset=['summary', 'story', 'id'], inplace=True)\n",
        "\n",
        "  # save the cleaned dataset\n",
        "  data_frame.to_csv(cleaned_dataset_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sAKv2B7yvor7"
      },
      "outputs": [],
      "source": [
        "original_dataset_path = '/content/drive/MyDrive/Colab Notebooks/dataset/original_dataset.csv'\n",
        "cleaned_dataset_path = '/content/drive/MyDrive/Colab Notebooks/dataset/cleaned_dataset.csv'\n",
        "preprocessed_dataset_path = '/content/drive/MyDrive/Colab Notebooks/dataset/preprocessed_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RGtH-hlxxejL"
      },
      "outputs": [],
      "source": [
        "remove_missing_rows(original_dataset_path, cleaned_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dw-uBVgqzUA_"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  # remove characters\n",
        "  text = re.sub(r'[^a-zA-Z\\s\\d]', '', text)\n",
        "\n",
        "  # tokenize the text\n",
        "  tokens = word_tokenize(text.lower())\n",
        "\n",
        "  # remove stop words\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  # stem tokens\n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  # lemmatize tokens\n",
        "  tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  # join tokens\n",
        "  tokens = ' '.join(tokens)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def preprocess_data_to_file(dataset_path, preprocessed_dataset_path):\n",
        "  df = pd.read_csv(dataset_path)\n",
        "  df['preprocessed_story'] = df['story'].apply(preprocess_text)\n",
        "  df['preprocessed_summary'] = df['summary'].apply(preprocess_text)\n",
        "  df.to_csv(preprocessed_dataset_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IVt_DjII0P1X"
      },
      "outputs": [],
      "source": [
        "preprocess_data_to_file(cleaned_dataset_path, preprocessed_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gHtOzp4a4BSj"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset_file):\n",
        "  df = pd.read_csv(dataset_file)\n",
        "\n",
        "  # split the dataset into training, validation, and testing sets\n",
        "  train_ratio = 0.8\n",
        "  val_ratio = 0.1\n",
        "  test_ratio = 0.1\n",
        "\n",
        "  train_df, remaining_df = train_test_split(df, test_size=1 - train_ratio, random_state=42)\n",
        "  val_df, test_df = train_test_split(remaining_df, test_size=test_ratio / (test_ratio + val_ratio), random_state=42)\n",
        "\n",
        "  train_df.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset/train_data.csv', index=False)\n",
        "  test_df.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset/test_data.csv', index=False)\n",
        "  val_df.to_csv('/content/drive/MyDrive/Colab Notebooks/dataset/val_data.csv', index=False)\n",
        "\n",
        "split_dataset(preprocessed_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6X-fZUxRzaBP"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "train_data = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/dataset/train_data.csv')\n",
        "val_data = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/dataset/val_data.csv')\n",
        "test_data = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/dataset/test_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Xy3H6RbozrT1"
      },
      "outputs": [],
      "source": [
        "# tokenize input and target sequence\n",
        "input_tokenizer = Tokenizer()\n",
        "input_tokenizer.fit_on_texts(train_data['preprocessed_story'])\n",
        "input_sequences = input_tokenizer.texts_to_sequences(train_data['preprocessed_story'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JmUER9mYzvra"
      },
      "outputs": [],
      "source": [
        "target_tokenizer = Tokenizer()\n",
        "target_tokenizer.fit_on_texts(train_data['preprocessed_summary'])\n",
        "target_sequences = target_tokenizer.texts_to_sequences(train_data['preprocessed_summary'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "c3y08k_pzycY"
      },
      "outputs": [],
      "source": [
        "# pad the input and target sequence\n",
        "max_input_length = 3000\n",
        "max_target_length = 100\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_target_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cps702x6z3Ry"
      },
      "outputs": [],
      "source": [
        "# define seq2seq model with attention\n",
        "embedding_dim = 100  # dimensionality of the embedding vectors\n",
        "hidden_units = 256  # number of hidden units in the LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "tTByB5Baz-We"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "encoder_inputs = Input(shape=(None, ))\n",
        "encoder_embedding = Embedding(len(input_tokenizer.word_index) + 1, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(hidden_units, return_sequences=False, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "q1F2hfJV0BN7"
      },
      "outputs": [],
      "source": [
        "# decoder\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "decoder_embedding = Embedding(len(target_tokenizer.word_index) + 1, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "39e7CUTi0ECr"
      },
      "outputs": [],
      "source": [
        "# attention\n",
        "attention_layer = Attention()\n",
        "context_vector = attention_layer([decoder_outputs, encoder_outputs])\n",
        "decoder_combined_context = tensorflow.concat([context_vector, decoder_outputs], axis=-1)\n",
        "decoder_dense = Dense(len(target_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_combined_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "00T_CW5G0HoU"
      },
      "outputs": [],
      "source": [
        "# define model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4htvKH620KZ9"
      },
      "outputs": [],
      "source": [
        "# compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxEg4M0P0NdM",
        "outputId": "6357a4dd-d5a2-436f-b686-bf1cae136a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "52/52 [==============================] - 1553s 30s/step - loss: 4.0043 - val_loss: 2.4759\n",
            "Epoch 2/10\n",
            "52/52 [==============================] - 1492s 29s/step - loss: 2.4226 - val_loss: 2.4091\n",
            "Epoch 3/10\n",
            "52/52 [==============================] - 1448s 28s/step - loss: 2.3509 - val_loss: 2.4071\n",
            "Epoch 4/10\n",
            "52/52 [==============================] - 1468s 28s/step - loss: 2.3323 - val_loss: 2.4189\n",
            "Epoch 5/10\n",
            "52/52 [==============================] - 1514s 29s/step - loss: 2.3187 - val_loss: 2.4326\n",
            "Epoch 6/10\n",
            "52/52 [==============================] - 1481s 29s/step - loss: 2.3062 - val_loss: 2.4353\n",
            "Epoch 7/10\n",
            "52/52 [==============================] - 1474s 28s/step - loss: 2.2960 - val_loss: 2.4427\n",
            "Epoch 8/10\n",
            "52/52 [==============================] - 1480s 29s/step - loss: 2.2852 - val_loss: 2.4589\n",
            "Epoch 9/10\n",
            "52/52 [==============================] - 1475s 28s/step - loss: 2.2732 - val_loss: 2.4590\n",
            "Epoch 10/10\n",
            "52/52 [==============================] - 1442s 28s/step - loss: 2.2602 - val_loss: 2.4468\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7be6115d2b90>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# train model\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "model.fit([input_sequences, target_sequences[:, :-1]], target_sequences[:, 1:], batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/models/seq2seq_attention_model.h5')"
      ],
      "metadata": {
        "id": "ASXx0pDc9tRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "CoHh--V85_7u"
      },
      "outputs": [],
      "source": [
        "# evaluation\n",
        "test_input_sequences = input_tokenizer.texts_to_sequences(test_data['preprocessed_story'])\n",
        "test_input_sequences = pad_sequences(test_input_sequences, maxlen=max_input_length, padding='post')\n",
        "test_target_sequences = target_tokenizer.texts_to_sequences(test_data['preprocessed_summary'])\n",
        "test_target_sequences = pad_sequences(test_target_sequences, maxlen=max_target_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiXeV3Oq6D_k",
        "outputId": "2f670255-cf9c-4497-a068-1e76fe7e3d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 86s 5s/step - loss: 2.1343\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model on test data\n",
        "test_loss = model.evaluate([test_input_sequences, test_target_sequences[:, :-1]], test_target_sequences[:, 1:])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}